{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$('.nbp-app-bar').toggle()"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$('.nbp-app-bar').toggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from markethistory import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_FEATURE = 3\n",
    "NUM_ASSET = 12\n",
    "OBS_WINDOW = 50\n",
    "\n",
    "EPISODE_WINDOW = 50\n",
    "\n",
    "TXN_FEE = 0.0025\n",
    "SAMPLING_BIAS = 1.3e-3 # This number needs to be carefully chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  3403.0755\n",
      "std:  734.204188765\n"
     ]
    }
   ],
   "source": [
    "# Find a good SAMPLING_BIAS\n",
    "\n",
    "def sample(start, end, bias):\n",
    "    \"\"\"\n",
    "    Geometrically sample a number in [START, END)\n",
    "    \n",
    "    Input:\n",
    "    - start: the start (inclusive)\n",
    "    - end: the end (exclusive)\n",
    "    - bias: a number between 0 to 1. The closer the bias to 1, the more\n",
    "      likely to generate a sample closer to END.\n",
    "    \"\"\"\n",
    "    offset = np.random.geometric(bias)\n",
    "    return max(end - offset, start)\n",
    "\n",
    "start, end = 50, 4166 # end is the total time length of training data\n",
    "data = [sample(start, end, 1.3e-3) for _ in range(2000)]\n",
    "plt.hist(data, 40)\n",
    "plt.axis([start, 4166, 0, 500])\n",
    "plt.show()\n",
    "print(\"mean: \", np.mean(data))\n",
    "print(\"std: \", np.std(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read price history from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: missing data for following coins ['DASH', 'FCT', 'GNT', 'ZEC']\n"
     ]
    }
   ],
   "source": [
    "start = '2017/12/17'\n",
    "end = '2018/04/20'\n",
    "markethistory = MarketHistory(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add constant cash (BTC) price info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global data tensor shape: (3, 12, 5951)\n"
     ]
    }
   ],
   "source": [
    "data_global = markethistory.data\n",
    "num_feature, num_asset, T = data_global.shape\n",
    "btc_price_tensor = np.ones((num_feature, 1, T))\n",
    "data_global = np.concatenate((btc_price_tensor, data_global), axis=1)\n",
    "print(\"Global data tensor shape:\", data_global.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into train, validataion, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data tensor shape:    torch.Size([3, 12, 4166])\n",
      "Validation data tensor shape:  torch.Size([3, 12, 1190])\n",
      "Testing data tensor shape:     torch.Size([3, 12, 595])\n"
     ]
    }
   ],
   "source": [
    "T_test = int(0.1 * T)\n",
    "T_valid = int(0.2 * T)\n",
    "T_train = T - T_test - T_valid\n",
    "\n",
    "data_global = torch.tensor(data_global, dtype=torch.float)\n",
    "data_train = data_global[:, :, :T_train]\n",
    "data_valid = data_global[:, :, T_train:T_train+T_valid]\n",
    "data_test = data_global[:, :, T_train+T_valid:]\n",
    "print(\"Training data tensor shape:   \", data_train.shape)\n",
    "print(\"Validation data tensor shape: \", data_valid.shape)\n",
    "print(\"Testing data tensor shape:    \", data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNetwork_CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    An EIIE style decision network implemented with CNN without separate\n",
    "    cash bias.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DecisionNetwork_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=NUM_FEATURE, \n",
    "                               out_channels=NUM_FEATURE, \n",
    "                               kernel_size=[1,NUM_FEATURE]) # can also use [1,2]\n",
    "        self.conv2 = nn.Conv2d(in_channels=NUM_FEATURE, \n",
    "                               out_channels=20, # can also use 10\n",
    "                               kernel_size=[1, OBS_WINDOW-2])\n",
    "        self.conv3 = nn.Conv2d(in_channels=21, \n",
    "                               out_channels=1, \n",
    "                               kernel_size=[1, 1])\n",
    "        \n",
    "    def forward(self, obs, prev_pf_w):\n",
    "        \"\"\"\n",
    "        Compute the forward pass. \n",
    "        \n",
    "        Input:\n",
    "        - obs: A fresh observation of the market environment at the current time step.\n",
    "          A tensor of shape [BATCH_SIZE, NUM_FEATURE, NUM_ASSET, OBS_WINDOW].\n",
    "        - prev_pf_w: The portfolio weight vector in the previous time step. A tensor\n",
    "          of shape [BATCH_SIZE, NUM_ASSET].\n",
    "        \n",
    "        Returns:\n",
    "        - new_pf_w: The new portfolio weight vector for the current time step. A tensor\n",
    "          of shape [BATCH_SIZE, NUM_ASSET]\n",
    "        \"\"\"\n",
    "        scores = nn.ReLU()(self.conv1(obs))\n",
    "        scores = nn.ReLU()(self.conv2(scores))\n",
    "        scores = torch.cat([scores, prev_pf_w.view(BATCH_SIZE, 1, NUM_ASSET, 1)], dim=1)\n",
    "        scores = self.conv3(scores).squeeze()\n",
    "        \n",
    "        new_pf_w = F.softmax(scores, dim=1)\n",
    "        return new_pf_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(start, end, bias):\n",
    "    \"\"\"\n",
    "    Geometrically sample a number in [START, END)\n",
    "    \n",
    "    Input:\n",
    "    - start: the start (inclusive)\n",
    "    - end: the end (exclusive)\n",
    "    - bias: a number between 0 to 1. The closer the bias to 1, the more\n",
    "      likely to generate a sample closer to END.\n",
    "    \"\"\"\n",
    "    offset = np.random.geometric(bias)\n",
    "    return max(end - offset, start)\n",
    "\n",
    "def sample_batch(batch_size, start, end, bias):\n",
    "    \"\"\"\n",
    "    Sample a batch of numbers geometrically distributed in [START, END)\n",
    "    \"\"\"\n",
    "    return torch.tensor([sample(start, end, bias) for _ in range(batch_size)])\n",
    "\n",
    "def get_observation(end_t_batch, history):\n",
    "    \"\"\"\n",
    "    Get a batch of price history of length OBS_WINDOW, ending at END_T_BATCH (inclusive).\n",
    "    \n",
    "    Input:\n",
    "    - end_t_batch: The end time indices of this observation. Shape: [BATCH_SIZE].\n",
    "    - history: The price history tensor of shape [NUM_FEATURE, NUM_ASSET, T]\n",
    "    \n",
    "    Returns:\n",
    "    - obs: A torch tensor of shape [BATCH_SIZE, NUM_FEATURE, NUM_ASSET, OBS_WINDOW]\n",
    "    \"\"\"\n",
    "    obs = []\n",
    "    for offset in range(OBS_WINDOW-1, -1, -1):\n",
    "        t_batch = end_t_batch - offset\n",
    "        observation = history[:, :, t_batch].permute(2, 0, 1)\n",
    "        obs.append(observation)\n",
    "    return torch.stack(obs, dim=-1)\n",
    "\n",
    "def calculate_shrinkage(w, w_prev):\n",
    "    \"\"\"\n",
    "    Calculate the porfolio value shrinkage during a portfolio weight re-allocation due\n",
    "    to transaction fees.\n",
    "    This function calculates the shrinkage using an iterative approximation method. See\n",
    "    equation (14) of the Deep Portfolio Management paper. \n",
    "    \n",
    "    Input:\n",
    "    - w: Target portfolio weight tensor of shape [BATCH_SIZE, NUM_ASSET]\n",
    "    - w_prev: Previous portfolio weight tensor of shape [BATCH_SIZE, NUM_ASSET]\n",
    "    \n",
    "    Returns:\n",
    "    - shrinkage: Portfolio value shrinkage multipler tensor of shape [BATCH_SIZE]\n",
    "    \"\"\"\n",
    "    w0_0, w0_m = w_prev[:, 0], w_prev[:, 1:]\n",
    "    w1_0, w1_m = w[:, 0], w[:, 1:]\n",
    "    \n",
    "    const1 = 1 - TXN_FEE * w0_0\n",
    "    const2 = 2 * TXN_FEE - TXN_FEE ** 2\n",
    "    const3 = 1 - TXN_FEE * w1_0\n",
    "    \n",
    "    u = TXN_FEE * torch.sum(torch.abs(w0_m - w1_m))\n",
    "    w1_m_T = w1_m.transpose(0, 1)\n",
    "    while True:\n",
    "        u_next = (const1 - const2*torch.sum(F.relu(w0_m - (u*w1_m_T).transpose(0,1)), dim=1)) / const3\n",
    "        max_diff = torch.max(torch.abs(u - u_next))\n",
    "        if max_diff <= 1e-10:\n",
    "            return u_next\n",
    "        u = u_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: instaed of fixed window size, try randomized window size\n",
    "# TODO: modify data matrix so that it includes a row of 1 for Cash\n",
    "# TODO: think of better way to initialize the initial pf weights\n",
    "\n",
    "def train(policy, data, lr=1e-3, episodes=10000):\n",
    "    optimizer = torch.optim.Adam(policy.parameters(), lr=lr)\n",
    "    T = data.shape[-1]\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        # geometrically sample start times: [batch]\n",
    "        start_indices = sample_batch(BATCH_SIZE, OBS_WINDOW, T-EPISODE_WINDOW, SAMPLING_BIAS)\n",
    "        # initialize portfolio weights: [batch, asset]\n",
    "        pf_w = (torch.ones(NUM_ASSETS) / NUM_ASSETS).repeat(BATCH_SIZE, 1)\n",
    "        # initialize portfolio values: [batch]\n",
    "        pf_v = torch.ones(BATCH_SIZE)\n",
    "        \n",
    "        # simulate one episode of live trading with the policy\n",
    "        loss = 0\n",
    "        price_curr = data[0, :, start_indices].transpose(0, 1) # [batch, asset]\n",
    "        for t in range(0, EPISODE_WINDOW):\n",
    "            price_next = data[0, :, start_indices+t+1].transpose(0, 1) # [batch, asset]\n",
    "            obs = get_observation(start_indices+t, data)\n",
    "            \n",
    "            pf_w_t_start = policy.forward(obs, pf_w)\n",
    "            shrinkage = calculate_shrinkage(pf_w_t_start, pf_w)\n",
    "            pf_v_t_start = pf_v * shrinkage\n",
    "            \n",
    "            w_tmp = (price_next / price_curr) * pf_w_t_start # [batch, asset]\n",
    "            w_tmp_sum = torch.sum(w_tmp, dim=1) # [batch]\n",
    "            pf_v_t_end = w_tmp_sum * pf_v_t_start\n",
    "            pf_w_t_end = w_tmp / w_tmp_sum.view(BATCH_SIZE, 1)\n",
    "            \n",
    "            batch_reward = torch.log(pf_v_t_end / pf_v)\n",
    "            loss -= torch.sum(batch_reward) / BATCH_SIZE\n",
    "            \n",
    "            # update variables\n",
    "            pf_w = pf_w_t_end\n",
    "            pf_v = pf_v_t_end\n",
    "            price_curr = price_next\n",
    "        loss /= EPISODE_WINDOW\n",
    "        \n",
    "        #if i %  == 0:\n",
    "        print(\"episode\", i, \" loss:\", float(loss))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE REAL DEAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = DecisionNetwork_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0  loss: 6.690469308523461e-05\n",
      "episode 1  loss: 7.6522228482645e-05\n",
      "episode 2  loss: 3.243281753384508e-06\n",
      "episode 3  loss: -4.191596599412151e-05\n",
      "episode 4  loss: 4.756957423524e-05\n",
      "episode 5  loss: 4.302796151023358e-05\n",
      "episode 6  loss: -1.973078542505391e-05\n",
      "episode 7  loss: 9.261535160476342e-05\n",
      "episode 8  loss: 5.934082582825795e-05\n",
      "episode 9  loss: 7.768180512357503e-05\n",
      "episode 10  loss: 0.0001297740964218974\n",
      "episode 11  loss: 6.129692337708548e-05\n",
      "episode 12  loss: 6.531638064188883e-05\n",
      "episode 13  loss: 8.538219117326662e-05\n",
      "episode 14  loss: 4.8699577746447176e-05\n",
      "episode 15  loss: 0.00010443062637932599\n",
      "episode 16  loss: 7.623177953064442e-05\n",
      "episode 17  loss: 9.700693772174418e-05\n",
      "episode 18  loss: 2.127499828930013e-05\n",
      "episode 19  loss: 2.2565222025150433e-05\n",
      "episode 20  loss: 6.155213486636057e-05\n",
      "episode 21  loss: 6.53225215501152e-05\n",
      "episode 22  loss: 0.00011391384759917855\n",
      "episode 23  loss: 0.0001416382729075849\n",
      "episode 24  loss: 4.9164038500748575e-05\n",
      "episode 25  loss: 9.769757889444008e-05\n",
      "episode 26  loss: 0.00014522552373819053\n",
      "episode 27  loss: 0.00010338075662730262\n",
      "episode 28  loss: 0.00011791858560172841\n",
      "episode 29  loss: 0.00012242938100825995\n",
      "episode 30  loss: -3.12963493342977e-05\n",
      "episode 31  loss: 8.687197987455875e-05\n",
      "episode 32  loss: 0.00010657896928023547\n",
      "episode 33  loss: 8.065417205216363e-05\n",
      "episode 34  loss: 9.982190385926515e-05\n",
      "episode 35  loss: 7.018610631348565e-05\n",
      "episode 36  loss: 8.732285641599447e-05\n",
      "episode 37  loss: 8.776384493103251e-05\n",
      "episode 38  loss: 0.00012545842037070543\n",
      "episode 39  loss: 6.929037772351876e-05\n",
      "episode 40  loss: 0.000154995039338246\n",
      "episode 41  loss: 4.832193371839821e-05\n",
      "episode 42  loss: -1.4511786503135227e-05\n",
      "episode 43  loss: 3.6917430406901985e-05\n",
      "episode 44  loss: 9.376984962727875e-05\n",
      "episode 45  loss: 2.0045836208737455e-05\n",
      "episode 46  loss: 0.00011246043140999973\n",
      "episode 47  loss: 2.7802088879980147e-05\n",
      "episode 48  loss: 1.9926710592699237e-06\n",
      "episode 49  loss: 0.00010306964395567775\n",
      "episode 50  loss: 0.00010863772331504151\n",
      "episode 51  loss: 3.109007957391441e-05\n",
      "episode 52  loss: 4.445930608198978e-05\n",
      "episode 53  loss: -7.4233871600881685e-06\n",
      "episode 54  loss: 0.00014022727555129677\n",
      "episode 55  loss: 9.870742360362783e-05\n",
      "episode 56  loss: 2.7612661142484285e-05\n",
      "episode 57  loss: 2.6802338197740028e-06\n",
      "episode 58  loss: 0.00010672092321328819\n",
      "episode 59  loss: 5.283077553031035e-05\n",
      "episode 60  loss: 0.00012107282236684114\n",
      "episode 61  loss: 1.5839003026485443e-05\n",
      "episode 62  loss: -3.0267913189163664e-06\n",
      "episode 63  loss: -3.514144191285595e-05\n",
      "episode 64  loss: 8.162137237377465e-05\n",
      "episode 65  loss: 8.898362284526229e-05\n",
      "episode 66  loss: -7.178756641224027e-05\n",
      "episode 67  loss: -4.388899014884373e-06\n",
      "episode 68  loss: 9.955428686225787e-05\n",
      "episode 69  loss: 0.00010475158342160285\n",
      "episode 70  loss: 0.00010962525266222656\n",
      "episode 71  loss: 0.00010559216752881184\n",
      "episode 72  loss: 4.131052628508769e-05\n",
      "episode 73  loss: 4.365721179055981e-05\n",
      "episode 74  loss: 5.544820669456385e-05\n",
      "episode 75  loss: 0.0001745556655805558\n",
      "episode 76  loss: 3.345257573528215e-05\n",
      "episode 77  loss: 4.4818847527494654e-05\n",
      "episode 78  loss: 6.977433713473147e-06\n",
      "episode 79  loss: 9.136024914369045e-07\n",
      "episode 80  loss: 9.094177585211582e-06\n",
      "episode 81  loss: 5.116676038596779e-05\n",
      "episode 82  loss: 0.0001301069132750854\n",
      "episode 83  loss: -7.503846291001537e-07\n",
      "episode 84  loss: -4.260766218067147e-05\n",
      "episode 85  loss: 1.0296948858012911e-05\n",
      "episode 86  loss: 6.358314567478374e-05\n",
      "episode 87  loss: 0.00013199962268117815\n",
      "episode 88  loss: 0.00010649072646629065\n",
      "episode 89  loss: 5.194335608393885e-05\n",
      "episode 90  loss: -5.7721150369616225e-05\n",
      "episode 91  loss: 5.646788849844597e-05\n",
      "episode 92  loss: -3.478528014966287e-05\n",
      "episode 93  loss: 7.330262451432645e-05\n",
      "episode 94  loss: 7.186665607150644e-05\n",
      "episode 95  loss: 5.0657639803830534e-05\n",
      "episode 96  loss: 4.69514743599575e-05\n",
      "episode 97  loss: 9.841439896263182e-05\n",
      "episode 98  loss: 5.284791041049175e-05\n",
      "episode 99  loss: -4.6509299863828346e-05\n",
      "episode 100  loss: 6.548628880409524e-05\n",
      "episode 101  loss: 0.00010031179408542812\n",
      "episode 102  loss: 7.3823073762469e-05\n",
      "episode 103  loss: 9.344939462607726e-05\n",
      "episode 104  loss: 8.058022649493068e-05\n",
      "episode 105  loss: 2.430523090879433e-05\n",
      "episode 106  loss: 0.00010554426989983767\n",
      "episode 107  loss: 9.138535824604332e-05\n",
      "episode 108  loss: 0.0001243433653144166\n",
      "episode 109  loss: 6.0048176237614825e-05\n",
      "episode 110  loss: 1.794144554878585e-05\n",
      "episode 111  loss: 7.622913108207285e-05\n",
      "episode 112  loss: 4.6639233914902434e-05\n",
      "episode 113  loss: 3.915035995305516e-05\n",
      "episode 114  loss: 3.242537422920577e-05\n",
      "episode 115  loss: 0.00010890533303609118\n",
      "episode 116  loss: 0.00011420073133194819\n",
      "episode 117  loss: 0.00011709727550623938\n",
      "episode 118  loss: 5.745427552028559e-05\n",
      "episode 119  loss: 0.00012912556121591479\n",
      "episode 120  loss: 3.998996180598624e-05\n",
      "episode 121  loss: 0.00012557847367133945\n",
      "episode 122  loss: 8.839988004183397e-05\n",
      "episode 123  loss: 1.675791850175301e-06\n",
      "episode 124  loss: 6.00321072852239e-05\n",
      "episode 125  loss: 0.00010851983824977651\n",
      "episode 126  loss: -2.861579014279414e-05\n",
      "episode 127  loss: 6.280701200012118e-05\n",
      "episode 128  loss: 4.319209256209433e-05\n",
      "episode 129  loss: 3.06423389702104e-05\n",
      "episode 130  loss: 2.1026144167990424e-05\n",
      "episode 131  loss: 9.523308835923672e-05\n",
      "episode 132  loss: 2.4840815967763774e-05\n",
      "episode 133  loss: 8.125902968458831e-05\n",
      "episode 134  loss: 5.6697826948948205e-05\n",
      "episode 135  loss: 0.00012901073205284774\n",
      "episode 136  loss: -2.033424152614316e-06\n",
      "episode 137  loss: 2.7913765734410845e-05\n",
      "episode 138  loss: 7.874046423239633e-05\n",
      "episode 139  loss: 6.516971916425973e-05\n",
      "episode 140  loss: 4.354225893621333e-05\n",
      "episode 141  loss: 7.289971108548343e-05\n",
      "episode 142  loss: 0.00012038480781484395\n",
      "episode 143  loss: 8.233868720708415e-05\n",
      "episode 144  loss: 5.252566552371718e-05\n",
      "episode 145  loss: 7.851938426028937e-05\n",
      "episode 146  loss: 6.842742732260376e-05\n",
      "episode 147  loss: 9.604284423403442e-05\n",
      "episode 148  loss: 7.864155486458912e-05\n",
      "episode 149  loss: 3.5425069654593244e-05\n",
      "episode 150  loss: 0.00011680423631332815\n",
      "episode 151  loss: 3.9897739043226466e-05\n",
      "episode 152  loss: 0.00010801425378303975\n",
      "episode 153  loss: 5.587143459706567e-05\n",
      "episode 154  loss: -3.142302739433944e-05\n",
      "episode 155  loss: 1.6403766494477168e-05\n",
      "episode 156  loss: 6.0876096540596336e-05\n",
      "episode 157  loss: 7.031825953163207e-05\n",
      "episode 158  loss: 3.994595681433566e-05\n",
      "episode 159  loss: 8.27943003969267e-05\n",
      "episode 160  loss: 6.294884951785207e-05\n",
      "episode 161  loss: 0.00010224890138488263\n",
      "episode 162  loss: 3.988515163655393e-05\n",
      "episode 163  loss: -1.1914017704839353e-05\n",
      "episode 164  loss: -5.2486117056105286e-05\n",
      "episode 165  loss: 4.307251947466284e-05\n",
      "episode 166  loss: 4.671777423936874e-05\n",
      "episode 167  loss: 5.024154597776942e-05\n",
      "episode 168  loss: 6.738511001458392e-05\n",
      "episode 169  loss: 3.383648072485812e-05\n",
      "episode 170  loss: 4.958879799232818e-05\n",
      "episode 171  loss: 4.9347370804753155e-05\n",
      "episode 172  loss: 8.303266076836735e-05\n",
      "episode 173  loss: 2.3877923922555055e-06\n",
      "episode 174  loss: 5.940366463619284e-06\n",
      "episode 175  loss: 4.037194958073087e-05\n",
      "episode 176  loss: 6.574492726940662e-05\n",
      "episode 177  loss: 3.8367739762179554e-05\n",
      "episode 178  loss: 9.084774501388893e-05\n",
      "episode 179  loss: 1.8705679394770414e-05\n",
      "episode 180  loss: 6.85239429003559e-05\n",
      "episode 181  loss: 6.53399110888131e-05\n",
      "episode 182  loss: 9.278854122385383e-05\n",
      "episode 183  loss: 5.699758912669495e-05\n",
      "episode 184  loss: 2.9629189157276414e-05\n",
      "episode 185  loss: 9.887519991025329e-05\n",
      "episode 186  loss: -1.0483144251338672e-05\n",
      "episode 187  loss: 2.791993028949946e-05\n",
      "episode 188  loss: 8.662160689709708e-05\n",
      "episode 189  loss: 7.229531911434606e-05\n",
      "episode 190  loss: 5.2893112297169864e-05\n",
      "episode 191  loss: 1.101610905607231e-05\n",
      "episode 192  loss: 3.0173569030012004e-05\n",
      "episode 193  loss: 6.916863640071824e-05\n",
      "episode 194  loss: 8.642547618364915e-05\n",
      "episode 195  loss: 1.8912000086857006e-05\n",
      "episode 196  loss: 0.00011846698907902464\n",
      "episode 197  loss: 6.914659752510488e-05\n",
      "episode 198  loss: 4.729943975689821e-05\n",
      "episode 199  loss: 4.2729239794425666e-05\n",
      "episode 200  loss: 3.625953468144871e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 201  loss: 7.836574513930827e-05\n",
      "episode 202  loss: 3.914897024515085e-05\n",
      "episode 203  loss: 3.992578058387153e-05\n",
      "episode 204  loss: 0.0001468007976654917\n",
      "episode 205  loss: -7.329944764933316e-06\n",
      "episode 206  loss: 6.782457057852298e-05\n",
      "episode 207  loss: 7.45283341530012e-06\n",
      "episode 208  loss: 7.633302448084578e-05\n",
      "episode 209  loss: 1.5869110939092934e-05\n",
      "episode 210  loss: 6.334218778647482e-05\n",
      "episode 211  loss: 2.3389726266032085e-05\n",
      "episode 212  loss: -3.31859992002137e-05\n",
      "episode 213  loss: 6.344612484099343e-05\n",
      "episode 214  loss: 4.2855557694565505e-05\n",
      "episode 215  loss: 6.856565596535802e-05\n",
      "episode 216  loss: 2.5477827875874937e-05\n",
      "episode 217  loss: 7.359744631685317e-05\n",
      "episode 218  loss: 9.284928091801703e-05\n",
      "episode 219  loss: 6.043410030542873e-05\n",
      "episode 220  loss: 9.413183761353139e-06\n",
      "episode 221  loss: 3.804539664997719e-05\n",
      "episode 222  loss: -5.76095089854789e-06\n",
      "episode 223  loss: 4.096447446499951e-05\n",
      "episode 224  loss: 2.8904200007673353e-05\n",
      "episode 225  loss: 4.52522435807623e-05\n",
      "episode 226  loss: 5.095755841466598e-05\n",
      "episode 227  loss: 5.2322149713290855e-05\n",
      "episode 228  loss: -4.300182990846224e-05\n",
      "episode 229  loss: 6.969726382521912e-05\n",
      "episode 230  loss: 7.428591197822243e-05\n",
      "episode 231  loss: 3.592412031139247e-05\n",
      "episode 232  loss: 1.0548125828790944e-05\n",
      "episode 233  loss: 2.6792215066961944e-05\n",
      "episode 234  loss: 4.708399501396343e-05\n",
      "episode 235  loss: 4.9614551244303584e-05\n",
      "episode 236  loss: 6.736262002959847e-05\n",
      "episode 237  loss: 8.629049261799082e-05\n",
      "episode 238  loss: 1.3726226825383492e-05\n",
      "episode 239  loss: 6.205785757629201e-05\n",
      "episode 240  loss: 8.726186933927238e-05\n",
      "episode 241  loss: 3.955444481107406e-05\n",
      "episode 242  loss: 0.00013277496327646077\n",
      "episode 243  loss: 1.5810344848432578e-05\n",
      "episode 244  loss: 2.2012172848917544e-05\n",
      "episode 245  loss: 2.1698218915844336e-05\n",
      "episode 246  loss: 3.696264320751652e-05\n",
      "episode 247  loss: -1.8241814814246027e-06\n",
      "episode 248  loss: 3.197422847733833e-05\n",
      "episode 249  loss: 5.462114859255962e-05\n",
      "episode 250  loss: 3.229967478546314e-05\n",
      "episode 251  loss: 4.951599839841947e-05\n",
      "episode 252  loss: 6.464511534431949e-05\n",
      "episode 253  loss: 3.088004814344458e-05\n",
      "episode 254  loss: 2.8819038561778143e-05\n",
      "episode 255  loss: 6.118118471931666e-05\n",
      "episode 256  loss: 6.732063775416464e-05\n",
      "episode 257  loss: 3.8547925214516e-05\n",
      "episode 258  loss: 5.24586794199422e-05\n",
      "episode 259  loss: 4.9250887968810275e-05\n",
      "episode 260  loss: 0.00011349755368428305\n",
      "episode 261  loss: 4.301890294300392e-05\n",
      "episode 262  loss: 6.7846108322555665e-06\n",
      "episode 263  loss: 0.000100970923085697\n",
      "episode 264  loss: 1.0071293218061328e-05\n",
      "episode 265  loss: 7.718605775153264e-05\n",
      "episode 266  loss: 6.747916631866246e-05\n",
      "episode 267  loss: 2.590498115750961e-05\n",
      "episode 268  loss: -5.701766713173129e-05\n",
      "episode 269  loss: 7.75460503064096e-05\n",
      "episode 270  loss: 2.5651310352259316e-05\n",
      "episode 271  loss: 2.5478750103502534e-05\n",
      "episode 272  loss: 9.551212133374065e-05\n",
      "episode 273  loss: 7.223200373118743e-05\n",
      "episode 274  loss: 7.591850589960814e-05\n",
      "episode 275  loss: 0.00010342877067159861\n",
      "episode 276  loss: 3.6747922422364354e-05\n",
      "episode 277  loss: 5.7665383792482316e-05\n",
      "episode 278  loss: 7.376135908998549e-05\n",
      "episode 279  loss: -1.1354341040714644e-05\n",
      "episode 280  loss: 2.5662226107669994e-05\n",
      "episode 281  loss: 3.97135918319691e-05\n",
      "episode 282  loss: 3.245632615289651e-05\n",
      "episode 283  loss: 0.00015469748177565634\n",
      "episode 284  loss: 4.23972487624269e-05\n",
      "episode 285  loss: 0.00012874268577434123\n",
      "episode 286  loss: 8.601306763011962e-05\n",
      "episode 287  loss: 7.968523277668282e-05\n",
      "episode 288  loss: 1.6413374396506697e-05\n",
      "episode 289  loss: 9.931161912390962e-05\n",
      "episode 290  loss: 7.911298598628491e-05\n",
      "episode 291  loss: 8.379246719414368e-05\n",
      "episode 292  loss: 1.693751255515963e-05\n",
      "episode 293  loss: 7.168997399276122e-05\n",
      "episode 294  loss: 0.0001024813755066134\n",
      "episode 295  loss: 6.994361319812015e-05\n",
      "episode 296  loss: 6.278201908571646e-05\n",
      "episode 297  loss: 2.788620076898951e-05\n",
      "episode 298  loss: 3.464865585556254e-05\n",
      "episode 299  loss: 2.164697798434645e-05\n",
      "episode 300  loss: 3.8378384488169104e-05\n",
      "episode 301  loss: 4.105636980966665e-05\n",
      "episode 302  loss: 5.0722759624477476e-05\n",
      "episode 303  loss: 5.5396991228917614e-05\n",
      "episode 304  loss: 3.802837454713881e-05\n",
      "episode 305  loss: 4.6048680815147236e-05\n",
      "episode 306  loss: 4.55477274954319e-05\n",
      "episode 307  loss: 9.065544145414606e-05\n",
      "episode 308  loss: 5.76280799577944e-05\n",
      "episode 309  loss: 4.0621485823066905e-05\n",
      "episode 310  loss: 1.8177664969698526e-05\n",
      "episode 311  loss: 5.219352533458732e-05\n",
      "episode 312  loss: 7.29540188331157e-05\n",
      "episode 313  loss: 1.2906315532745793e-05\n",
      "episode 314  loss: 8.804314711596817e-05\n",
      "episode 315  loss: 4.77659996249713e-05\n",
      "episode 316  loss: 3.093516352237202e-05\n",
      "episode 317  loss: 5.4486034059664235e-05\n",
      "episode 318  loss: 1.4940493201720528e-05\n",
      "episode 319  loss: 5.544620580621995e-05\n",
      "episode 320  loss: 5.9230962506262586e-05\n",
      "episode 321  loss: 5.1757604524027556e-05\n",
      "episode 322  loss: 0.00010053455480374396\n",
      "episode 323  loss: 2.722634053498041e-05\n",
      "episode 324  loss: 2.9699185688514262e-05\n",
      "episode 325  loss: 6.591770215891302e-05\n",
      "episode 326  loss: 9.606835374142975e-05\n",
      "episode 327  loss: 2.73229325102875e-05\n",
      "episode 328  loss: 4.321050801081583e-05\n",
      "episode 329  loss: 3.600017953431234e-05\n",
      "episode 330  loss: 7.11191605660133e-05\n",
      "episode 331  loss: 2.7098603823105805e-05\n",
      "episode 332  loss: 3.245151674491353e-05\n",
      "episode 333  loss: 3.5517372452886775e-05\n",
      "episode 334  loss: 5.674285421264358e-05\n",
      "episode 335  loss: 5.7038632803596556e-05\n",
      "episode 336  loss: 5.7280169130535796e-05\n",
      "episode 337  loss: 2.233482700830791e-05\n",
      "episode 338  loss: 3.922339965356514e-05\n",
      "episode 339  loss: 7.619167445227504e-05\n",
      "episode 340  loss: 6.149144610390067e-05\n",
      "episode 341  loss: 4.582224937621504e-05\n",
      "episode 342  loss: 1.6333731764461845e-05\n",
      "episode 343  loss: 5.353759479476139e-05\n",
      "episode 344  loss: 7.927136175567284e-05\n",
      "episode 345  loss: 2.860158019757364e-05\n",
      "episode 346  loss: 4.620346589945257e-05\n",
      "episode 347  loss: 4.454735972103663e-05\n",
      "episode 348  loss: 6.101996041252278e-05\n",
      "episode 349  loss: 4.109965811949223e-05\n",
      "episode 350  loss: 4.652822462958284e-05\n",
      "episode 351  loss: 7.028086838545278e-05\n",
      "episode 352  loss: 7.423642819048837e-05\n",
      "episode 353  loss: 2.6066138161695562e-05\n",
      "episode 354  loss: 8.197312854463235e-05\n",
      "episode 355  loss: 6.73415997880511e-05\n",
      "episode 356  loss: 4.180061660008505e-05\n",
      "episode 357  loss: 8.156349213095382e-05\n",
      "episode 358  loss: 9.948133083526045e-06\n",
      "episode 359  loss: 7.018315955065191e-05\n",
      "episode 360  loss: 2.0752988348249346e-05\n",
      "episode 361  loss: 6.083913103793748e-05\n",
      "episode 362  loss: 5.533318471862003e-05\n",
      "episode 363  loss: 5.6961889640660957e-05\n",
      "episode 364  loss: 6.146358646219596e-05\n",
      "episode 365  loss: 5.432199395727366e-05\n",
      "episode 366  loss: 5.520126433111727e-05\n",
      "episode 367  loss: 2.065543048956897e-05\n",
      "episode 368  loss: 7.588410517200828e-05\n",
      "episode 369  loss: 4.9756752559915185e-05\n",
      "episode 370  loss: 3.721938992384821e-05\n",
      "episode 371  loss: 6.444602331612259e-05\n",
      "episode 372  loss: 4.570247256197035e-05\n",
      "episode 373  loss: 6.296615902101621e-05\n",
      "episode 374  loss: 4.595548671204597e-05\n",
      "episode 375  loss: 5.0848339014919475e-05\n",
      "episode 376  loss: 4.238285691826604e-05\n",
      "episode 377  loss: 7.320460281334817e-05\n",
      "episode 378  loss: 5.0509894208516926e-05\n",
      "episode 379  loss: 4.155905844527297e-05\n",
      "episode 380  loss: 5.979465277050622e-05\n",
      "episode 381  loss: 5.735411832574755e-05\n",
      "episode 382  loss: 2.608796967251692e-05\n",
      "episode 383  loss: 4.507152334554121e-05\n",
      "episode 384  loss: 2.0379909983603284e-05\n",
      "episode 385  loss: 4.6887398639228195e-05\n",
      "episode 386  loss: 4.5191565732238814e-05\n",
      "episode 387  loss: 5.1760500355158e-05\n",
      "episode 388  loss: 4.974242256139405e-05\n",
      "episode 389  loss: 5.232486728345975e-05\n",
      "episode 390  loss: 4.861698835156858e-05\n",
      "episode 391  loss: 6.170672713778913e-05\n",
      "episode 392  loss: 5.056331065134145e-05\n",
      "episode 393  loss: 6.04776905674953e-05\n",
      "episode 394  loss: 4.856773739447817e-05\n",
      "episode 395  loss: 2.376354495936539e-05\n",
      "episode 396  loss: 5.823399260407314e-05\n",
      "episode 397  loss: 3.1469870009459555e-05\n",
      "episode 398  loss: 6.266497075557709e-05\n",
      "episode 399  loss: 6.55577823636122e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 400  loss: 5.317647446645424e-05\n",
      "episode 401  loss: 5.992987280478701e-05\n",
      "episode 402  loss: 5.285409861244261e-05\n",
      "episode 403  loss: 4.928608541376889e-05\n",
      "episode 404  loss: 4.2813051550183445e-05\n",
      "episode 405  loss: 5.525749656953849e-05\n",
      "episode 406  loss: 5.21124093211256e-05\n",
      "episode 407  loss: 4.839567918679677e-05\n",
      "episode 408  loss: 2.751818101387471e-05\n",
      "episode 409  loss: 3.405866300454363e-05\n",
      "episode 410  loss: 3.29538706864696e-05\n",
      "episode 411  loss: 4.437793177203275e-05\n",
      "episode 412  loss: 4.8404730478068814e-05\n",
      "episode 413  loss: 4.2719486373243853e-05\n",
      "episode 414  loss: 4.189428000245243e-05\n",
      "episode 415  loss: 4.978070137440227e-05\n",
      "episode 416  loss: 4.700002318713814e-05\n",
      "episode 417  loss: 6.172951543703675e-05\n",
      "episode 418  loss: 4.181489930488169e-05\n",
      "episode 419  loss: 3.411990837776102e-05\n",
      "episode 420  loss: 6.097877485444769e-05\n",
      "episode 421  loss: 3.940136593882926e-05\n",
      "episode 422  loss: 6.833284714957699e-05\n",
      "episode 423  loss: 3.886605554725975e-05\n",
      "episode 424  loss: 6.339302490232512e-05\n",
      "episode 425  loss: 3.900967203662731e-05\n",
      "episode 426  loss: 2.9345541406655684e-05\n",
      "episode 427  loss: 2.140916149073746e-05\n",
      "episode 428  loss: 6.127967208158225e-05\n",
      "episode 429  loss: 4.087312481715344e-05\n",
      "episode 430  loss: 6.454260437749326e-05\n",
      "episode 431  loss: 4.115517731406726e-05\n",
      "episode 432  loss: 4.839219764107838e-05\n",
      "episode 433  loss: 5.222620893619023e-05\n",
      "episode 434  loss: 5.5472519306931645e-05\n",
      "episode 435  loss: 3.508363079163246e-05\n",
      "episode 436  loss: 4.457372415345162e-05\n",
      "episode 437  loss: 5.14959501742851e-05\n",
      "episode 438  loss: 5.380161019274965e-05\n",
      "episode 439  loss: 5.875331044080667e-05\n",
      "episode 440  loss: 3.4724376746453345e-05\n",
      "episode 441  loss: 4.362977051641792e-05\n",
      "episode 442  loss: 5.0346439820714295e-05\n",
      "episode 443  loss: 3.660534639493562e-05\n",
      "episode 444  loss: 5.4694708524039015e-05\n",
      "episode 445  loss: 4.215310764266178e-05\n",
      "episode 446  loss: 3.83077967853751e-05\n",
      "episode 447  loss: 4.2103667510673404e-05\n",
      "episode 448  loss: 4.505610922933556e-05\n",
      "episode 449  loss: 5.41903791599907e-05\n",
      "episode 450  loss: 5.3768391808262095e-05\n",
      "episode 451  loss: 4.252145663485862e-05\n",
      "episode 452  loss: 5.442017572931945e-05\n",
      "episode 453  loss: 5.2072471589781344e-05\n",
      "episode 454  loss: 6.83136167936027e-05\n",
      "episode 455  loss: 4.053986049257219e-05\n",
      "episode 456  loss: 4.609468305716291e-05\n",
      "episode 457  loss: 4.9843216402223334e-05\n",
      "episode 458  loss: 5.365351171349175e-05\n",
      "episode 459  loss: 5.6181681429734454e-05\n",
      "episode 460  loss: 4.924122185911983e-05\n",
      "episode 461  loss: 4.349888695287518e-05\n",
      "episode 462  loss: 4.480450297705829e-05\n",
      "episode 463  loss: 4.758562135975808e-05\n",
      "episode 464  loss: 5.613299072138034e-05\n",
      "episode 465  loss: 4.1870247514452785e-05\n",
      "episode 466  loss: 4.852792699239217e-05\n",
      "episode 467  loss: 5.226051507634111e-05\n",
      "episode 468  loss: 4.283766247681342e-05\n",
      "episode 469  loss: 4.4066393456887454e-05\n",
      "episode 470  loss: 4.150134191149846e-05\n",
      "episode 471  loss: 4.492201696848497e-05\n",
      "episode 472  loss: 5.684468487743288e-05\n",
      "episode 473  loss: 3.9356913475785404e-05\n",
      "episode 474  loss: 5.1263337809359655e-05\n",
      "episode 475  loss: 2.9415268727461807e-05\n",
      "episode 476  loss: 5.2046390919713303e-05\n",
      "episode 477  loss: 4.8851063183974475e-05\n",
      "episode 478  loss: 4.567621726891957e-05\n",
      "episode 479  loss: 4.294044629205018e-05\n",
      "episode 480  loss: 4.676748358178884e-05\n",
      "episode 481  loss: 4.175087815383449e-05\n",
      "episode 482  loss: 5.5108670494519174e-05\n",
      "episode 483  loss: 4.542457463685423e-05\n",
      "episode 484  loss: 5.011797838960774e-05\n",
      "episode 485  loss: 4.506764162215404e-05\n",
      "episode 486  loss: 4.946457193000242e-05\n",
      "episode 487  loss: 4.505096876528114e-05\n",
      "episode 488  loss: 5.27520569448825e-05\n",
      "episode 489  loss: 4.090736911166459e-05\n",
      "episode 490  loss: 5.072711064713076e-05\n",
      "episode 491  loss: 5.4164738685358316e-05\n",
      "episode 492  loss: 5.709481774829328e-05\n",
      "episode 493  loss: 4.823181006941013e-05\n",
      "episode 494  loss: 5.493753633345477e-05\n",
      "episode 495  loss: 3.6398509109858423e-05\n",
      "episode 496  loss: 5.2613373554777354e-05\n",
      "episode 497  loss: 5.132940714247525e-05\n",
      "episode 498  loss: 4.6762259444221854e-05\n",
      "episode 499  loss: 4.441744022187777e-05\n",
      "episode 500  loss: 4.162357799941674e-05\n",
      "episode 501  loss: 4.509503560257144e-05\n",
      "episode 502  loss: 5.022510958951898e-05\n",
      "episode 503  loss: 4.9928439693758264e-05\n",
      "episode 504  loss: 3.412142177694477e-05\n",
      "episode 505  loss: 5.362090087146498e-05\n",
      "episode 506  loss: 4.989662193111144e-05\n",
      "episode 507  loss: 4.506047844188288e-05\n",
      "episode 508  loss: 4.8382185923401266e-05\n",
      "episode 509  loss: 4.938882193528116e-05\n",
      "episode 510  loss: 5.178500941838138e-05\n",
      "episode 511  loss: 4.5585573388962075e-05\n",
      "episode 512  loss: 4.59174916613847e-05\n",
      "episode 513  loss: 4.454944428289309e-05\n",
      "episode 514  loss: 4.170066677033901e-05\n",
      "episode 515  loss: 5.171183875063434e-05\n",
      "episode 516  loss: 4.5642940676771104e-05\n",
      "episode 517  loss: 4.398990495246835e-05\n",
      "episode 518  loss: 4.6479566663037986e-05\n",
      "episode 519  loss: 4.4612399506149814e-05\n",
      "episode 520  loss: 4.6391527575906366e-05\n",
      "episode 521  loss: 5.6468459661118686e-05\n",
      "episode 522  loss: 4.049896597280167e-05\n",
      "episode 523  loss: 3.7599722418235615e-05\n",
      "episode 524  loss: 4.764184996020049e-05\n",
      "episode 525  loss: 4.660931517719291e-05\n",
      "episode 526  loss: 4.722911398857832e-05\n",
      "episode 527  loss: 4.954532778356224e-05\n",
      "episode 528  loss: 4.581722532748245e-05\n",
      "episode 529  loss: 3.8830545236123726e-05\n",
      "episode 530  loss: 5.0947499403264374e-05\n",
      "episode 531  loss: 5.3644915169570595e-05\n",
      "episode 532  loss: 4.149605592829175e-05\n",
      "episode 533  loss: 4.968877328792587e-05\n",
      "episode 534  loss: 3.781509440159425e-05\n",
      "episode 535  loss: 5.433722981251776e-05\n",
      "episode 536  loss: 6.144001963548362e-05\n",
      "episode 537  loss: 3.3268177503487095e-05\n",
      "episode 538  loss: 3.55158990714699e-05\n",
      "episode 539  loss: 5.572865848080255e-05\n",
      "episode 540  loss: 5.052462802268565e-05\n",
      "episode 541  loss: 5.3621723054675385e-05\n",
      "episode 542  loss: 4.6675409976160154e-05\n",
      "episode 543  loss: 5.016943396185525e-05\n",
      "episode 544  loss: 4.316844933782704e-05\n",
      "episode 545  loss: 4.747392449644394e-05\n",
      "episode 546  loss: 4.186153819318861e-05\n",
      "episode 547  loss: 4.9252026656176895e-05\n",
      "episode 548  loss: 4.0276961954077706e-05\n",
      "episode 549  loss: 4.4712392991641536e-05\n",
      "episode 550  loss: 4.097931014257483e-05\n",
      "episode 551  loss: 5.2305898861959577e-05\n",
      "episode 552  loss: 4.0472354157827795e-05\n",
      "episode 553  loss: 4.661340790335089e-05\n",
      "episode 554  loss: 4.707687548943795e-05\n",
      "episode 555  loss: 5.367804624256678e-05\n",
      "episode 556  loss: 4.697734038927592e-05\n",
      "episode 557  loss: 5.098297697259113e-05\n",
      "episode 558  loss: 4.8254947614623234e-05\n",
      "episode 559  loss: 4.214135333313607e-05\n",
      "episode 560  loss: 4.319282743381336e-05\n",
      "episode 561  loss: 4.815403008251451e-05\n",
      "episode 562  loss: 5.285256702336483e-05\n",
      "episode 563  loss: 4.6465484047075734e-05\n",
      "episode 564  loss: 4.979708683094941e-05\n",
      "episode 565  loss: 4.33614150097128e-05\n",
      "episode 566  loss: 4.57968344562687e-05\n",
      "episode 567  loss: 4.522050585364923e-05\n",
      "episode 568  loss: 3.570676199160516e-05\n",
      "episode 569  loss: 4.6401997678913176e-05\n",
      "episode 570  loss: 5.167621202417649e-05\n",
      "episode 571  loss: 4.3877105781575665e-05\n",
      "episode 572  loss: 3.9380705857183784e-05\n",
      "episode 573  loss: 4.632081254385412e-05\n",
      "episode 574  loss: 5.1824343245243654e-05\n",
      "episode 575  loss: 4.889162664767355e-05\n",
      "episode 576  loss: 4.850919867749326e-05\n",
      "episode 577  loss: 3.7767131289001554e-05\n",
      "episode 578  loss: 4.969927613274194e-05\n",
      "episode 579  loss: 5.3074472816661e-05\n",
      "episode 580  loss: 3.961949187214486e-05\n",
      "episode 581  loss: 5.0651346100494266e-05\n",
      "episode 582  loss: 4.6937333536334336e-05\n",
      "episode 583  loss: 4.961546801496297e-05\n",
      "episode 584  loss: 4.1576899093342945e-05\n",
      "episode 585  loss: 4.277773041394539e-05\n",
      "episode 586  loss: 4.4475513277575374e-05\n",
      "episode 587  loss: 5.2664723625639454e-05\n",
      "episode 588  loss: 5.326274913386442e-05\n",
      "episode 589  loss: 4.486921170610003e-05\n",
      "episode 590  loss: 4.820231333724223e-05\n",
      "episode 591  loss: 4.6677236241521314e-05\n",
      "episode 592  loss: 4.494061795412563e-05\n",
      "episode 593  loss: 5.416491694631986e-05\n",
      "episode 594  loss: 4.699877536040731e-05\n",
      "episode 595  loss: 4.881048880633898e-05\n",
      "episode 596  loss: 4.611533222487196e-05\n",
      "episode 597  loss: 4.227674435242079e-05\n",
      "episode 598  loss: 3.8568858144572005e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 599  loss: 4.60338887933176e-05\n",
      "episode 600  loss: 4.0836017433321103e-05\n",
      "episode 601  loss: 4.599442399921827e-05\n",
      "episode 602  loss: 5.243182022240944e-05\n",
      "episode 603  loss: 4.716190596809611e-05\n",
      "episode 604  loss: 5.114440500619821e-05\n",
      "episode 605  loss: 4.9881418817676604e-05\n",
      "episode 606  loss: 4.8695186706027016e-05\n",
      "episode 607  loss: 4.87421675643418e-05\n",
      "episode 608  loss: 4.6580466005252674e-05\n",
      "episode 609  loss: 4.8955524107441306e-05\n",
      "episode 610  loss: 4.696052201325074e-05\n",
      "episode 611  loss: 4.3528292735572904e-05\n",
      "episode 612  loss: 4.476202593650669e-05\n",
      "episode 613  loss: 4.464577796170488e-05\n",
      "episode 614  loss: 4.527339115156792e-05\n",
      "episode 615  loss: 4.4089760194765404e-05\n",
      "episode 616  loss: 4.639030521502718e-05\n",
      "episode 617  loss: 4.9580965423956513e-05\n",
      "episode 618  loss: 4.99252273584716e-05\n",
      "episode 619  loss: 4.434854781720787e-05\n",
      "episode 620  loss: 4.749164145323448e-05\n",
      "episode 621  loss: 4.528869976638816e-05\n",
      "episode 622  loss: 5.1142815209459513e-05\n",
      "episode 623  loss: 5.3359748562797904e-05\n",
      "episode 624  loss: 4.7088105930015445e-05\n",
      "episode 625  loss: 4.5452427002601326e-05\n",
      "episode 626  loss: 4.686668034992181e-05\n",
      "episode 627  loss: 4.605096182785928e-05\n",
      "episode 628  loss: 4.053375596413389e-05\n",
      "episode 629  loss: 4.749494110001251e-05\n",
      "episode 630  loss: 4.6510627726092935e-05\n",
      "episode 631  loss: 4.840760084334761e-05\n",
      "episode 632  loss: 4.2839194065891206e-05\n",
      "episode 633  loss: 4.804621130460873e-05\n",
      "episode 634  loss: 4.428622196428478e-05\n",
      "episode 635  loss: 4.977679782314226e-05\n",
      "episode 636  loss: 4.9416445108363405e-05\n",
      "episode 637  loss: 4.863614230998792e-05\n",
      "episode 638  loss: 4.703444574261084e-05\n",
      "episode 639  loss: 5.22239861311391e-05\n",
      "episode 640  loss: 4.406588777783327e-05\n",
      "episode 641  loss: 4.580145832733251e-05\n",
      "episode 642  loss: 5.202343163546175e-05\n",
      "episode 643  loss: 4.759385410579853e-05\n",
      "episode 644  loss: 4.5975441025802866e-05\n",
      "episode 645  loss: 4.781291863764636e-05\n",
      "episode 646  loss: 4.9303074774798006e-05\n",
      "episode 647  loss: 4.6102482883725315e-05\n",
      "episode 648  loss: 4.8349676944781095e-05\n",
      "episode 649  loss: 4.350299423094839e-05\n",
      "episode 650  loss: 4.8552210500929505e-05\n",
      "episode 651  loss: 4.530366641120054e-05\n",
      "episode 652  loss: 4.724219616036862e-05\n",
      "episode 653  loss: 4.635799996322021e-05\n",
      "episode 654  loss: 4.966584674548358e-05\n",
      "episode 655  loss: 4.7456069296458736e-05\n",
      "episode 656  loss: 4.626211375580169e-05\n",
      "episode 657  loss: 4.787576472153887e-05\n",
      "episode 658  loss: 4.512488521868363e-05\n",
      "episode 659  loss: 4.471195279620588e-05\n",
      "episode 660  loss: 4.7306450142059475e-05\n",
      "episode 661  loss: 5.036656875745393e-05\n",
      "episode 662  loss: 4.769231964019127e-05\n",
      "episode 663  loss: 4.916267789667472e-05\n",
      "episode 664  loss: 4.9324691644869745e-05\n",
      "episode 665  loss: 4.779481241712347e-05\n",
      "episode 666  loss: 4.651226117857732e-05\n",
      "episode 667  loss: 4.388502929941751e-05\n",
      "episode 668  loss: 4.479670315049589e-05\n",
      "episode 669  loss: 4.749123036162928e-05\n",
      "episode 670  loss: 4.353990880190395e-05\n",
      "episode 671  loss: 4.423019345267676e-05\n",
      "episode 672  loss: 4.794905544258654e-05\n",
      "episode 673  loss: 4.3846783228218555e-05\n",
      "episode 674  loss: 4.49478866357822e-05\n",
      "episode 675  loss: 4.68855032522697e-05\n",
      "episode 676  loss: 4.622674168786034e-05\n",
      "episode 677  loss: 4.741076554637402e-05\n",
      "episode 678  loss: 4.6400458813877776e-05\n",
      "episode 679  loss: 4.986562271369621e-05\n",
      "episode 680  loss: 4.385977081255987e-05\n",
      "episode 681  loss: 5.163303285371512e-05\n",
      "episode 682  loss: 4.6846918849041685e-05\n",
      "episode 683  loss: 4.6626784751424566e-05\n",
      "episode 684  loss: 4.506548066274263e-05\n",
      "episode 685  loss: 4.524973701336421e-05\n",
      "episode 686  loss: 4.6241984819062054e-05\n",
      "episode 687  loss: 4.637978054233827e-05\n",
      "episode 688  loss: 4.685141539084725e-05\n",
      "episode 689  loss: 4.627692396752536e-05\n",
      "episode 690  loss: 4.581723260344006e-05\n",
      "episode 691  loss: 4.5833825424779207e-05\n",
      "episode 692  loss: 4.7146360884653404e-05\n",
      "episode 693  loss: 4.8212801630143076e-05\n",
      "episode 694  loss: 4.7649758926127106e-05\n",
      "episode 695  loss: 4.644846194423735e-05\n",
      "episode 696  loss: 4.602553235599771e-05\n",
      "episode 697  loss: 4.60646660940256e-05\n",
      "episode 698  loss: 4.56758716609329e-05\n",
      "episode 699  loss: 4.864195943810046e-05\n",
      "episode 700  loss: 4.6157383621903136e-05\n",
      "episode 701  loss: 4.801679096999578e-05\n",
      "episode 702  loss: 4.53711545560509e-05\n",
      "episode 703  loss: 4.7337562136817724e-05\n",
      "episode 704  loss: 4.786349745700136e-05\n",
      "episode 705  loss: 4.4923508539795876e-05\n",
      "episode 706  loss: 4.8171059461310506e-05\n",
      "episode 707  loss: 4.7106579586397856e-05\n",
      "episode 708  loss: 4.58353097201325e-05\n",
      "episode 709  loss: 4.7080699005164206e-05\n",
      "episode 710  loss: 4.669120971811935e-05\n",
      "episode 711  loss: 4.650509072234854e-05\n",
      "episode 712  loss: 4.605157664627768e-05\n",
      "episode 713  loss: 4.375712160253897e-05\n",
      "episode 714  loss: 4.563296170090325e-05\n",
      "episode 715  loss: 4.555679697659798e-05\n",
      "episode 716  loss: 4.595800783135928e-05\n",
      "episode 717  loss: 4.710620851255953e-05\n",
      "episode 718  loss: 4.7201589040923864e-05\n",
      "episode 719  loss: 4.652250572689809e-05\n",
      "episode 720  loss: 4.587518924381584e-05\n",
      "episode 721  loss: 4.605371577781625e-05\n",
      "episode 722  loss: 4.744514444610104e-05\n",
      "episode 723  loss: 4.6049583033891395e-05\n",
      "episode 724  loss: 4.4227293983567506e-05\n",
      "episode 725  loss: 4.594817437464371e-05\n",
      "episode 726  loss: 4.4987918954575434e-05\n",
      "episode 727  loss: 4.793998959939927e-05\n",
      "episode 728  loss: 4.641861596610397e-05\n",
      "episode 729  loss: 4.533789251581766e-05\n",
      "episode 730  loss: 4.736731352750212e-05\n",
      "episode 731  loss: 4.503261880017817e-05\n",
      "episode 732  loss: 4.7801746404729784e-05\n",
      "episode 733  loss: 4.82017348986119e-05\n",
      "episode 734  loss: 4.5246928493725136e-05\n",
      "episode 735  loss: 4.5342421799432486e-05\n",
      "episode 736  loss: 4.6606844989582896e-05\n",
      "episode 737  loss: 4.362920662970282e-05\n",
      "episode 738  loss: 4.604280547937378e-05\n",
      "episode 739  loss: 4.573757905745879e-05\n",
      "episode 740  loss: 4.511750375968404e-05\n",
      "episode 741  loss: 4.653885116567835e-05\n",
      "episode 742  loss: 4.620420440915041e-05\n",
      "episode 743  loss: 4.611906479112804e-05\n",
      "episode 744  loss: 4.701896250480786e-05\n",
      "episode 745  loss: 4.708507185569033e-05\n",
      "episode 746  loss: 4.6077817387413234e-05\n",
      "episode 747  loss: 4.736521441373043e-05\n",
      "episode 748  loss: 4.6713525080122054e-05\n",
      "episode 749  loss: 4.6784072765149176e-05\n",
      "episode 750  loss: 4.4314241677057e-05\n",
      "episode 751  loss: 4.55283880000934e-05\n",
      "episode 752  loss: 4.6444343752227724e-05\n",
      "episode 753  loss: 4.745940532302484e-05\n",
      "episode 754  loss: 4.458035982679576e-05\n",
      "episode 755  loss: 4.683544102590531e-05\n",
      "episode 756  loss: 4.488144622882828e-05\n",
      "episode 757  loss: 4.9098129238700494e-05\n",
      "episode 758  loss: 4.5546243200078607e-05\n",
      "episode 759  loss: 4.478437767829746e-05\n",
      "episode 760  loss: 4.595766222337261e-05\n",
      "episode 761  loss: 4.620434629032388e-05\n",
      "episode 762  loss: 4.459143747226335e-05\n",
      "episode 763  loss: 4.479515700950287e-05\n",
      "episode 764  loss: 4.5069278712617233e-05\n",
      "episode 765  loss: 4.680228084907867e-05\n",
      "episode 766  loss: 4.4409702240955085e-05\n",
      "episode 767  loss: 4.7401794290635735e-05\n",
      "episode 768  loss: 4.532015009317547e-05\n",
      "episode 769  loss: 4.58820941275917e-05\n",
      "episode 770  loss: 4.668449400924146e-05\n",
      "episode 771  loss: 4.623760105459951e-05\n",
      "episode 772  loss: 4.786073259310797e-05\n",
      "episode 773  loss: 4.4905147660756484e-05\n",
      "episode 774  loss: 4.47555175924208e-05\n",
      "episode 775  loss: 4.6810404455754906e-05\n",
      "episode 776  loss: 4.485351382754743e-05\n",
      "episode 777  loss: 4.53757202194538e-05\n",
      "episode 778  loss: 4.542144597508013e-05\n",
      "episode 779  loss: 4.615265061147511e-05\n",
      "episode 780  loss: 4.69608603452798e-05\n",
      "episode 781  loss: 4.683488805312663e-05\n",
      "episode 782  loss: 4.67308891529683e-05\n",
      "episode 783  loss: 4.4399268517736346e-05\n",
      "episode 784  loss: 4.759109651786275e-05\n",
      "episode 785  loss: 4.783962504006922e-05\n",
      "episode 786  loss: 4.584430644172244e-05\n",
      "episode 787  loss: 4.5396136556519195e-05\n",
      "episode 788  loss: 4.4627744500758126e-05\n",
      "episode 789  loss: 4.616602018359117e-05\n",
      "episode 790  loss: 4.7022207581903785e-05\n",
      "episode 791  loss: 4.6639659558422863e-05\n",
      "episode 792  loss: 4.530445585260168e-05\n",
      "episode 793  loss: 4.567713040160015e-05\n",
      "episode 794  loss: 4.903655644739047e-05\n",
      "episode 795  loss: 4.6168719563866034e-05\n",
      "episode 796  loss: 4.702327714767307e-05\n",
      "episode 797  loss: 4.626814916264266e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 798  loss: 4.379397432785481e-05\n",
      "episode 799  loss: 4.642839849111624e-05\n",
      "episode 800  loss: 4.58707581856288e-05\n",
      "episode 801  loss: 4.621908374247141e-05\n",
      "episode 802  loss: 4.415580406202935e-05\n",
      "episode 803  loss: 4.6175136958481744e-05\n",
      "episode 804  loss: 4.571893805405125e-05\n",
      "episode 805  loss: 4.4954842451261356e-05\n",
      "episode 806  loss: 4.6062330511631444e-05\n",
      "episode 807  loss: 4.6801200369372964e-05\n",
      "episode 808  loss: 4.7163070121314377e-05\n",
      "episode 809  loss: 4.553294274955988e-05\n",
      "episode 810  loss: 4.740964141092263e-05\n",
      "episode 811  loss: 4.6634453610749915e-05\n",
      "episode 812  loss: 4.5869564928580076e-05\n",
      "episode 813  loss: 4.58277172583621e-05\n",
      "episode 814  loss: 4.65824814455118e-05\n",
      "episode 815  loss: 4.694938979810104e-05\n",
      "episode 816  loss: 4.618159437086433e-05\n",
      "episode 817  loss: 4.5830012822989374e-05\n",
      "episode 818  loss: 4.60899573226925e-05\n",
      "episode 819  loss: 4.5679877075599506e-05\n",
      "episode 820  loss: 4.5173488615546376e-05\n",
      "episode 821  loss: 4.5897522795712575e-05\n",
      "episode 822  loss: 4.5918786781840026e-05\n",
      "episode 823  loss: 4.5688673708355054e-05\n",
      "episode 824  loss: 4.6632590965600684e-05\n",
      "episode 825  loss: 4.70025661343243e-05\n",
      "episode 826  loss: 4.579099913826212e-05\n",
      "episode 827  loss: 4.731473381980322e-05\n",
      "episode 828  loss: 4.670208363677375e-05\n",
      "episode 829  loss: 4.573992919176817e-05\n",
      "episode 830  loss: 4.646320303436369e-05\n",
      "episode 831  loss: 4.5477947423933074e-05\n",
      "episode 832  loss: 4.655410157283768e-05\n",
      "episode 833  loss: 4.648187314160168e-05\n",
      "episode 834  loss: 4.5787892304360867e-05\n",
      "episode 835  loss: 4.5855482312617823e-05\n",
      "episode 836  loss: 4.531327067525126e-05\n",
      "episode 837  loss: 4.7153891500784084e-05\n",
      "episode 838  loss: 4.650490518542938e-05\n",
      "episode 839  loss: 4.4900454668095335e-05\n",
      "episode 840  loss: 4.673546209232882e-05\n",
      "episode 841  loss: 4.603054912877269e-05\n",
      "episode 842  loss: 4.704300226876512e-05\n",
      "episode 843  loss: 4.5725209929514676e-05\n",
      "episode 844  loss: 4.679109770222567e-05\n",
      "episode 845  loss: 4.6804114390397444e-05\n",
      "episode 846  loss: 4.6593886509072036e-05\n",
      "episode 847  loss: 4.583127156365663e-05\n",
      "episode 848  loss: 4.631869524018839e-05\n",
      "episode 849  loss: 4.596764119924046e-05\n",
      "episode 850  loss: 4.695605457527563e-05\n",
      "episode 851  loss: 4.432514469954185e-05\n",
      "episode 852  loss: 4.638767859432846e-05\n",
      "episode 853  loss: 4.750005609821528e-05\n",
      "episode 854  loss: 4.590565004036762e-05\n",
      "episode 855  loss: 4.5614942791871727e-05\n",
      "episode 856  loss: 4.690969944931567e-05\n",
      "episode 857  loss: 4.647364403354004e-05\n",
      "episode 858  loss: 4.533414903562516e-05\n",
      "episode 859  loss: 4.517821071203798e-05\n",
      "episode 860  loss: 4.558599175652489e-05\n",
      "episode 861  loss: 4.6468860091408715e-05\n",
      "episode 862  loss: 4.6250275772763416e-05\n",
      "episode 863  loss: 4.647269452107139e-05\n",
      "episode 864  loss: 4.588759475154802e-05\n",
      "episode 865  loss: 4.648841422749683e-05\n",
      "episode 866  loss: 4.553091639536433e-05\n",
      "episode 867  loss: 4.4749482185579836e-05\n",
      "episode 868  loss: 4.5194130507297814e-05\n",
      "episode 869  loss: 4.651931521948427e-05\n",
      "episode 870  loss: 4.562993854051456e-05\n",
      "episode 871  loss: 4.682644794229418e-05\n",
      "episode 872  loss: 4.756153066409752e-05\n",
      "episode 873  loss: 4.601429827744141e-05\n",
      "episode 874  loss: 4.696079486166127e-05\n",
      "episode 875  loss: 4.602600529324263e-05\n",
      "episode 876  loss: 4.653532960219309e-05\n",
      "episode 877  loss: 4.514727334026247e-05\n",
      "episode 878  loss: 4.547934076981619e-05\n",
      "episode 879  loss: 4.499320493778214e-05\n",
      "episode 880  loss: 4.625125802704133e-05\n",
      "episode 881  loss: 4.4623204303206876e-05\n",
      "episode 882  loss: 4.67842910438776e-05\n",
      "episode 883  loss: 4.6662687964271754e-05\n",
      "episode 884  loss: 4.609471943695098e-05\n",
      "episode 885  loss: 4.520891161519103e-05\n",
      "episode 886  loss: 4.565535709843971e-05\n",
      "episode 887  loss: 4.6517307055182755e-05\n",
      "episode 888  loss: 4.6114204451441765e-05\n",
      "episode 889  loss: 4.650028859032318e-05\n",
      "episode 890  loss: 4.615641228156164e-05\n",
      "episode 891  loss: 4.5028435124550015e-05\n",
      "episode 892  loss: 4.633994467440061e-05\n",
      "episode 893  loss: 4.585328497341834e-05\n",
      "episode 894  loss: 4.610506948665716e-05\n",
      "episode 895  loss: 4.564792834571563e-05\n",
      "episode 896  loss: 4.7534653276670724e-05\n",
      "episode 897  loss: 4.6969809773145244e-05\n",
      "episode 898  loss: 4.483906377572566e-05\n",
      "episode 899  loss: 4.646301385946572e-05\n",
      "episode 900  loss: 4.651036215364002e-05\n",
      "episode 901  loss: 4.6744746214244515e-05\n",
      "episode 902  loss: 4.590905155055225e-05\n",
      "episode 903  loss: 4.6351386117748916e-05\n",
      "episode 904  loss: 4.542610258795321e-05\n",
      "episode 905  loss: 4.5403128751786426e-05\n",
      "episode 906  loss: 4.547940625343472e-05\n",
      "episode 907  loss: 4.58477770735044e-05\n",
      "episode 908  loss: 4.688880289904773e-05\n",
      "episode 909  loss: 4.4797368900617585e-05\n",
      "episode 910  loss: 4.646128581953235e-05\n",
      "episode 911  loss: 4.624862049240619e-05\n",
      "episode 912  loss: 4.706647087004967e-05\n",
      "episode 913  loss: 4.5913220674265176e-05\n",
      "episode 914  loss: 4.6690471208421513e-05\n",
      "episode 915  loss: 4.582373730954714e-05\n",
      "episode 916  loss: 4.484581222641282e-05\n",
      "episode 917  loss: 4.6036984713282436e-05\n",
      "episode 918  loss: 4.498374255490489e-05\n",
      "episode 919  loss: 4.6541110350517556e-05\n",
      "episode 920  loss: 4.6391192881856114e-05\n",
      "episode 921  loss: 4.7770248784217983e-05\n",
      "episode 922  loss: 4.677883043768816e-05\n",
      "episode 923  loss: 4.507573612499982e-05\n",
      "episode 924  loss: 4.640181577997282e-05\n",
      "episode 925  loss: 4.622593041858636e-05\n",
      "episode 926  loss: 4.642014391720295e-05\n",
      "episode 927  loss: 4.5212855184217915e-05\n",
      "episode 928  loss: 4.5605898776557297e-05\n",
      "episode 929  loss: 4.5830085582565516e-05\n",
      "episode 930  loss: 4.599554813466966e-05\n",
      "episode 931  loss: 4.5771033910568804e-05\n",
      "episode 932  loss: 4.514557804213837e-05\n",
      "episode 933  loss: 4.5582783059217036e-05\n",
      "episode 934  loss: 4.6592009312007576e-05\n",
      "episode 935  loss: 4.583410918712616e-05\n",
      "episode 936  loss: 4.6828794438624755e-05\n",
      "episode 937  loss: 4.632806303561665e-05\n",
      "episode 938  loss: 4.6954770368756726e-05\n",
      "episode 939  loss: 4.5852804760215804e-05\n",
      "episode 940  loss: 4.5699682232225314e-05\n",
      "episode 941  loss: 4.6092689444776624e-05\n",
      "episode 942  loss: 4.647944660973735e-05\n",
      "episode 943  loss: 4.6753677452215925e-05\n",
      "episode 944  loss: 4.546624040813185e-05\n",
      "episode 945  loss: 4.597021688823588e-05\n",
      "episode 946  loss: 4.540120789897628e-05\n",
      "episode 947  loss: 4.622890264727175e-05\n",
      "episode 948  loss: 4.5750341087114066e-05\n",
      "episode 949  loss: 4.585549686453305e-05\n",
      "episode 950  loss: 4.71176317660138e-05\n",
      "episode 951  loss: 4.574461854645051e-05\n",
      "episode 952  loss: 4.6019609726499766e-05\n",
      "episode 953  loss: 4.6028941142139956e-05\n",
      "episode 954  loss: 4.7282170271500945e-05\n",
      "episode 955  loss: 4.644891669158824e-05\n",
      "episode 956  loss: 4.6762557758484036e-05\n",
      "episode 957  loss: 4.539235669653863e-05\n",
      "episode 958  loss: 4.568427175399847e-05\n",
      "episode 959  loss: 4.584779526339844e-05\n",
      "episode 960  loss: 4.5850276364944875e-05\n",
      "episode 961  loss: 4.631056071957573e-05\n",
      "episode 962  loss: 4.579644519253634e-05\n",
      "episode 963  loss: 4.520215225056745e-05\n",
      "episode 964  loss: 4.623848508344963e-05\n",
      "episode 965  loss: 4.604754576575942e-05\n",
      "episode 966  loss: 4.707628977485001e-05\n",
      "episode 967  loss: 4.62725838588085e-05\n",
      "episode 968  loss: 4.5838754886062816e-05\n",
      "episode 969  loss: 4.61299714515917e-05\n",
      "episode 970  loss: 4.621636981028132e-05\n",
      "episode 971  loss: 4.6355038648471236e-05\n",
      "episode 972  loss: 4.6318164095282555e-05\n",
      "episode 973  loss: 4.601425462169573e-05\n",
      "episode 974  loss: 4.611312397173606e-05\n",
      "episode 975  loss: 4.6358982217498124e-05\n",
      "episode 976  loss: 4.650333721656352e-05\n",
      "episode 977  loss: 4.593415360432118e-05\n",
      "episode 978  loss: 4.63245996797923e-05\n",
      "episode 979  loss: 4.564368646242656e-05\n",
      "episode 980  loss: 4.5339522330323234e-05\n",
      "episode 981  loss: 4.6138782636262476e-05\n",
      "episode 982  loss: 4.631385309039615e-05\n",
      "episode 983  loss: 4.661730054067448e-05\n",
      "episode 984  loss: 4.541677117231302e-05\n",
      "episode 985  loss: 4.485106546781026e-05\n",
      "episode 986  loss: 4.616173100657761e-05\n",
      "episode 987  loss: 4.546318086795509e-05\n",
      "episode 988  loss: 4.56096968264319e-05\n",
      "episode 989  loss: 4.583570989780128e-05\n",
      "episode 990  loss: 4.5449527533492073e-05\n",
      "episode 991  loss: 4.5742275688098744e-05\n",
      "episode 992  loss: 4.528457429842092e-05\n",
      "episode 993  loss: 4.6150165871949866e-05\n",
      "episode 994  loss: 4.5967150072101504e-05\n",
      "episode 995  loss: 4.6358192776096985e-05\n",
      "episode 996  loss: 4.5234184653963894e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 997  loss: 4.641208943212405e-05\n",
      "episode 998  loss: 4.5148935896577314e-05\n",
      "episode 999  loss: 4.5207201765151694e-05\n"
     ]
    }
   ],
   "source": [
    "train(policy, data_train, lr=0.00028, episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeprlbootcamp]",
   "language": "python",
   "name": "conda-env-deeprlbootcamp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
