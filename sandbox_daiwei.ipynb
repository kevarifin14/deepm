{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from networks import *\n",
    "from markethistory import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$('.nbp-app-bar').toggle()"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$('.nbp-app-bar').toggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ASSETS = 11\n",
    "OBS_WINDOW = 50\n",
    "EPISOD_WINDOW = 50\n",
    "TXN_FEE = 0.0025\n",
    "SAMPLING_BIAS = 5e-3\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: missing data for following coins ['DASH', 'FCT', 'GNT', 'ZEC']\n"
     ]
    }
   ],
   "source": [
    "start = '2018/04/10'\n",
    "end = '2018/04/20'\n",
    "markethistory = MarketHistory(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 11, 337)\n",
      "(3, 11, 96)\n",
      "(3, 11, 48)\n"
     ]
    }
   ],
   "source": [
    "N_global = markethistory.data.shape[2]\n",
    "N_test = int(0.1 * N)\n",
    "N_valid = int(0.2 * N)\n",
    "N_train = N - test_N - valid_N\n",
    "\n",
    "train_data = markethistory.data[:, :, :N_train]\n",
    "valid_data = markethistory.data[:, :, N_train:N_train + N_valid]\n",
    "test_data = markethistory.data[:, :, N_train + N_valid:]\n",
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(start, end, bias):\n",
    "    \"\"\"\n",
    "    Geometrically sample a number in [START, END)\n",
    "    \n",
    "    Input:\n",
    "    - start: the start (inclusive)\n",
    "    - end: the end (exclusive)\n",
    "    - bias: a number between 0 to 1. The closer the bias to 1, the more\n",
    "      likely to generate a sample closer to END.\n",
    "    \"\"\"\n",
    "    offset = np.random.geometric(bias)\n",
    "    while offset > end - start:\n",
    "        offset = np.random.geometric(bias)\n",
    "    t = end - offset\n",
    "    return t\n",
    "\n",
    "def sample_fast(start, end, bias):\n",
    "    \"\"\"\n",
    "    Geometrically sample a number in [START, END)\n",
    "    \n",
    "    Input:\n",
    "    - start: the start (inclusive)\n",
    "    - end: the end (exclusive)\n",
    "    - bias: a number between 0 to 1. The closer the bias to 1, the more\n",
    "      likely to generate a sample closer to END.\n",
    "    \"\"\"\n",
    "    offset = np.random.geometric(bias)\n",
    "    return max(end - offset, start)\n",
    "\n",
    "def sample_batch(batch_size, start, end, bias):\n",
    "    return np.array([sample_fast(start, end, bias) for _ in range(batch_size)])\n",
    "\n",
    "def get_observation(end_t_batch, data):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - obs: A torch tensor of shape [batch, feature, asset, OBS_WINDOW]\n",
    "    \"\"\"\n",
    "    obs = []\n",
    "    for offset in range(OBS_WINDOW-1, -1, -1):\n",
    "        t_batch = end_t_batch - offset\n",
    "        observation = data[:, :, t_batch].permute(2, 0, 1)\n",
    "        obs.append(observation)\n",
    "    return torch.stack(obs, dim=-1)\n",
    "\n",
    "def calculate_mu(w, w_prev):\n",
    "    w0_0, w0_m = w_prev[:, 0], w_prev[:, 1:]\n",
    "    w1_0, w1_m = w[:, 0], w[:, 1:]\n",
    "    c = TXN_FEE\n",
    "    \n",
    "    const1 = 1 - c*w0_0\n",
    "    const2 = 2*c - c*c\n",
    "    const3 = 1 - c*w1_0\n",
    "    \n",
    "    u = c * torch.sum(torch.abs(w0_m - w1_m))\n",
    "    w1_m_T = w1_m.transpose(0, 1)\n",
    "    while True:\n",
    "        u_next = (const1 - const2*torch.sum(F.relu(w0_m - (u*w1_m_T).transpose(0,1)), dim=1)) / const3\n",
    "        max_diff = torch.max(torch.abs(u - u_next))\n",
    "        if max_diff <= 1e-10:\n",
    "            return u_next\n",
    "        u = u_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9977,  0.9972])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([[0.5,0.25,0.05,0.1,0.0,0.1],[0.2,0.05,0.17,0.08,0.25,0.25]])\n",
    "w_prev = torch.tensor([[0.5,0.,0.5,0.,0.,0.],[0.2,0.,0.,0.,0.8,0.]])\n",
    "calculate_mu(w, w_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: instaed of fixed window size, try randomized window size\n",
    "# TODO: modify data matrix so that it includes a row of 1 for Cash\n",
    "# TODO: think of better way to initialize the initial pf weights\n",
    "def train(policy, data, lr, episods=10000):\n",
    "    optimizer = torch.optim.Adam(policy.parameters(), lr=lr)\n",
    "    T = data.shape[-1]\n",
    "    \n",
    "    for i in range(episods):\n",
    "        # geometrically sample start times: [batch]\n",
    "        start_indices = sample_batch(BATCH_SIZE, OBS_WINDOW, T-EPISOD_WINDOW, SAMPLING_BIAS)\n",
    "        # initialize portfolio weights: [batch, asset]\n",
    "        pf_w = (torch.ones(NUM_ASSETS) / NUM_ASSETS).repeat(BATCH_SIZE, 1)\n",
    "        # initialize portfolio values: [batch]\n",
    "        pf_v = torch.ones(BATCH_SIZE)\n",
    "        \n",
    "        # simulate one episod of live trading with the policy\n",
    "        loss = 0\n",
    "        price_curr = data[0, :, start_indices] # [batch, asset]\n",
    "        for t in range(0, EPISOD_WINDOW):\n",
    "            price_next = data[0, :, start_indicies+t+1] # [batch, asset]\n",
    "            obs = get_observation(start_indices+t, data)\n",
    "            \n",
    "            pf_w_t_start = policy.forward(obs, pf_w)\n",
    "            shrinkage = calculate_shrinkage(pf_w_t_start, pf_w)\n",
    "            pf_v_t_start = pf_v * shrinkage\n",
    "            \n",
    "            w_tmp = (price_next / price_curr) * pf_w_t_start # [batch, asset]\n",
    "            w_tmp_sum = torch.sum(w_tmp, dim=1) # [batch]\n",
    "            pf_v_t_end = w_tmp_sum * pf_v_t_start\n",
    "            pf_w_t_end = w_tmp / w_tmp_sum.view(BATCH_SIZE, 1)\n",
    "            \n",
    "            batch_reward = torch.log(pf_v_t_end / pf_v)\n",
    "            loss -= torch.sum(batch_reward) / BATCH_SIZE\n",
    "            \n",
    "            # update variables\n",
    "            pf_w = pf_w_t_end\n",
    "            pf_v = pf_v_t_end\n",
    "            price_curr = price_next\n",
    "        loss /= EPISOD_WINDOW\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = DecisionNetworks_CNN(asset=NUM_ASSETS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeprlbootcamp]",
   "language": "python",
   "name": "conda-env-deeprlbootcamp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
